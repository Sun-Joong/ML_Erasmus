{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look\n",
    "In this notebook we will do the \"Hello world!\" of deep learning (DL). Do not worry if you do not understand all the steps performed in the notebook. Setting up a network in the beginning can seem complicated, but we will explain all steps later in the session and during the rest of the course. This notebook is only meant to show how \"things look and work\" in practice, and show you the typical steps involved in solving a problem with deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will perform **classification** on a dataset containing 70000 images of handwritten digits (the MNIST dataset). Our task is to **classify** each handwritten digit as a number from 0 to 9. Every image in the dataset has already been correctly **labelled** as one of these numbers and we use these labels to guide the training.\n",
    "\n",
    "Let's look at some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The MNIST dataset](images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "The dataset is built in to the **Keras** library, so let us start by loading the library and the dataset. Please select the cell below, and either run it by pressing `Shift + Enter`, or with the run button at the top of the notebook looking like this: ![The Run buttton](images/run-button.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "\n",
    "mnist <- dataset_mnist()\n",
    "train_images <- mnist$train$x\n",
    "train_labels <- mnist$train$y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the **dimensions** of the training images using R's [`dim`](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/dim) command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the first dimension is typically used for **instances** or **samples**. That is: we have 60.000 training images (samples) available.\n",
    "\n",
    "The remaining dimensions are the dimensions for each particular instance. We can see from the output above that each instance has two dimensions corresponding to the size of the image (28 by 28 pixels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the training *labels*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 60.000 labels as well, each of them corresponding to an image. This data is one-dimensional, since we only need a single number as a label. To get a feel for the labels, we use the [`str`](https://www.rdocumentation.org/packages/utils/versions/3.5.1/topics/str) command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the labels start from 0 and, since we have 10 classes, end at 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the first image as a matrix to get a sense of how the images are represented numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.matrix.max.cols=28, repr.matrix.max.rows=28)  # This will tell R to show the full matrix\n",
    "train_images[1,,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a `5` digit appear in the matrix, with the background (black) represented as 0, and white as 255. Let's check the label of this sample to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having inspected the data we now define the model, or network. The network will take an image as its input and output a label. At this stage it is not important to understand how this happens or what the lines below do exactly.\n",
    "\n",
    "The single important thing to notice is how we use the Keras library and that we define two **dense** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()\n",
    "model %>%\n",
    "  layer_dense(units = 512, activation = \"sigmoid\", input_shape = c(28 * 28)) %>%\n",
    "  layer_dense(units = 10, activation = \"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level the first layer will learn to **represent** the data and the second layer will output labels, 0-9. This is what deep learning is all about, putting layers on top of layers and then produce some output.\n",
    "\n",
    "Usually, the first layers will **compress** raw data to useful **features**, which bypasses a lot of work (*feature engineering*) required for conventional machine learning (ML) techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, optimizer and metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we make sure that the output of the model makes sense and produces good results? Put differently, how do we give feedback to the model's predictions? This is done during **training**. During training the model is fed an image and it will output a label. By providing the actual network output and the expected output (in this case a label between 0 and 9) to a **loss function**, we can calculate a **loss value**, or simply **loss**.\n",
    "\n",
    "We seek to minimize this loss by updating the **parameters** of the model. The **optimizer** takes care of updating the parameters based on the loss function. Usually, the model processes a number of images in a **batch** and then performs one update **step**. Little by little the model starts performing better. \n",
    "\n",
    "The loss function is generally not easy to relate to any evaluation criterion that we as humans may understand. To relate the network performance to a number understandable by us, we define a **metric**.\n",
    "In the next cell we define an optimizer, a loss function and a metric which measures the performance of our model, in this case **accuracy**. For the time being, ignore the values given to the optimizer and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    optimizer = \"rmsprop\",\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run our data through the model and start training it, we need to make small adjustments to the data. At this stage it is not important to understand what the lines below do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images <- array_reshape(train_images, c(60000, 28 * 28))\n",
    "train_images <- train_images / 255\n",
    "train_labels <- to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the dimensions of the preprocessed training data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Compare the dimensions of the data with the ones from the original data set. Can you guess how the data has changed, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN YOUR ANSWER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training the model. We do this by calling the Keras [`fit`](https://keras.rstudio.com/reference/fit.html) function. Since neural networks are incrementally updated we often train using the same **samples** many times. The **epoch** parameter controls how many times we run the training set through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model %>% fit(\n",
    "    train_images, train_labels, \n",
    "    epochs = 10, \n",
    "    batch_size = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the **loss** and the **accuracy** of the trained model after each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the loss decreases steadily, and the accuracy of the model improves after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, you can see that the training accuracy is more than 99%. This is very high, of course, but not necessarily a good measure of the actual performance of the model when it encounters samples it has not seen before.\n",
    "\n",
    "To properly evaluate the performance of our model, we need a **test dataset**. Let's gather the test dataset and adjust it like we did with the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images <- mnist$test$x\n",
    "test_labels <- mnist$test$y\n",
    "\n",
    "test_images <- array_reshape(test_images, c(10000, 28 * 28))\n",
    "test_images <- test_images / 255\n",
    "test_labels <- to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 10000 images to test our model's performance on. Let's do that with the Keras [`evaluate`](https://www.rdocumentation.org/packages/keras/versions/0.3.8/topics/evaluate) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics <- model %>% evaluate(test_images, test_labels)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, performance is still great but lower than on our training set.\n",
    "\n",
    "The reason is that the model is **overfitting**, a concept that we will explore more closely in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise (optional)\n",
    "Try to improve the training accuracy by increasing the number of epochs little by little. What effects do you see on the training and test accuracy? Can you think of a reason why you see these effects?\n",
    "\n",
    "We have provided you with the necessary code in the cell below, so you can run it to train and evaluate the model.\n",
    "\n",
    "**NOTE: please be aware that each epoch takes around one second, so try not to use too many epochs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "model <- keras_model_sequential()\n",
    "model %>%\n",
    "  layer_dense(units = 512, activation = \"sigmoid\", input_shape = c(28 * 28)) %>%\n",
    "  layer_dense(units = 10, activation = \"softmax\")\n",
    "\n",
    "# Compile it\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = \"rmsprop\",\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")\n",
    "\n",
    "# Train it\n",
    "\n",
    "history <- model %>% fit(\n",
    "    train_images, train_labels, \n",
    "    epochs = 100, \n",
    "    batch_size = 1024\n",
    ")\n",
    "\n",
    "# Plot the training accuracy and loss\n",
    "\n",
    "plot(history)\n",
    "\n",
    "# Show the loss and accuracy on the test set\n",
    "\n",
    "model %>% evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
