{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fashion MNIST problem with Keras\n",
    "In this notebook you will solve a binary variant of the Fashion MNIST problem. The original Fashion MNIST dataset contains images for 10 different classes of clothing. We have selected two classes from this dataset for you to work on.\n",
    "\n",
    "You will preprocess the data to get it into the right shape to process with Keras, and see how overfitting and underfitting shows itself in the loss and accuracy curves during training.\n",
    "\n",
    "Let's load Keras and some helper functions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "source(\"02-helpers.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- dataset_fashion_mnist_binary()\n",
    "data_train <- data$train\n",
    "data_test <- data$test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Inspect the dimensions of the training and test data. What do the different dimensions represent for the `x` and `y` variables inside each set?\n",
    "\n",
    "**Hint**: first inspect the contents of data_train and data_test using the [`names`](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/names) function. This function will show you the names of the variables inside of an R object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot one of the samples in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "\n",
    "index <- 6000\n",
    "options(repr.plot.width = 3, repr.plot.height = 3)\n",
    "ggplot(melt(t(apply(data_train$x[index,,], 2, rev)), varnames=c('x', 'y')), aes(x=x, y=y, fill=value)) +\n",
    "    geom_raster() +\n",
    "    scale_x_continuous(expand = c(0, 0)) +\n",
    "    scale_y_continuous(expand = c(0, 0)) +\n",
    "    scale_fill_gradient(low=\"#000000\", high=\"#FFFFFF\") +\n",
    "    theme_void() +\n",
    "    theme(legend.position = \"none\") +\n",
    "    ggtitle(paste('Label:', data_train$y[index]))\n",
    "options(repr.plot.width = 6, repr.plot.height = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "The input images have pixel values that are not in the range 0 to 1. Rescale them such that all values are in this range.\n",
    "\n",
    "**Hint**: use R's `min` and `max` functions to find out the minimum and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Divide the original training data 80%-20% between the new training and validation set. Make sure that you slice both the images and the labels, and verify the dimensions of the two sets using `dim` and `length`.\n",
    "\n",
    "**Hints**:\n",
    "1. You will need four variables for storing the images and labels of your two sets.\n",
    "1. Think carefully about the dimensions of the data, and how it affects the slicing.\n",
    "1. **If you are unable to solve this exercise, please continue with the next one, where we have done it for you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cell below only if you did not complete exercise 2 or 3\n",
    "If you couldn't slice the training and validation set yourself, please run the following code to make sure you have the preprocessed data ready. If not, please continue to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- dataset_fashion_mnist_binary()\n",
    "data_train <- data$train\n",
    "data_test <- data$test\n",
    "\n",
    "# Exercise 2\n",
    "index <- 0.8 * length(data_train$y)\n",
    "dim(data_train$y)\n",
    "x_val <- data_train$x[-(1:index),,]\n",
    "y_val <- data_train$y[-(1:index)]\n",
    "\n",
    "dim(x_val)\n",
    "dim(y_val)\n",
    "\n",
    "x_train <- data_train$x[1:index,,]\n",
    "y_train <- data_train$y[1:index]\n",
    "\n",
    "dim(x_train)\n",
    "dim(y_train)\n",
    "\n",
    "# Exercise 3\n",
    "min(x_train)\n",
    "max(x_train)\n",
    "x_train <- x_train / 255  # these are images in the range 0 to 255, so we can just divide by 255\n",
    "x_val <- x_val / 255 # same for the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Create an initial model by filling out the skeleton below. We have provided you with the first two layers. The first layer is a **flatten** layer, which will convert the two-dimensional 28 x 28 image into a one-dimensional vector of length 784.\n",
    "\n",
    "Print a summary of the model and verify the output shapes of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    layer_flatten(input_shape = c(28, 28)) %>%\n",
    "    <FILL IN>\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Compile the model by filling out the skeleton below according to the following requirements:\n",
    "1. The optimizer should be `optimizer_adam` with learning rate 0.001\n",
    "1. The loss should be `binary_crossentropy`\n",
    "1. The metric should be accuracy (warning: Keras expects a character vector)\n",
    "\n",
    "**Hint**: it may help to keep the previous notebook open for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    optimizer = <FILL IN>,\n",
    "    loss = <FILL IN>,\n",
    "    metrics = <FILL IN>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Fit the model and plot the training history by filling out the skeleton below. Use 250 epochs.\n",
    "\n",
    "Do you think the model has sufficient capacity to model the problem. Why or why not? Motivate your answer in a few sentences in the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model %>% fit(\n",
    "    x = <FILL IN>,\n",
    "    y = <FILL IN>,\n",
    "    epochs = <FILL IN>,\n",
    "    batch_size = 1024,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "Add the validation set to your training process and increase the model's capacity by adding more neurons and/or layers. Make sure that the model has enough capacity to solve the problem. We have provided you with a skeleton to fill out below. You can modify the learning rate, number of epochs and the network architecture yourself.\n",
    "\n",
    "Where does the model start overfitting? Motivate your answer in the second cell below as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    layer_flatten(input_shape = c(28, 28)) %>%\n",
    "    <FILL IN>\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.001),\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    validation_data = list(<FILL IN THE VALIDATION IMAGES, FILL IN THE VALIDATION LABELS>),\n",
    "    epochs = 500,\n",
    "    batch_size = 1024,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **evaluate** our model on the test set to obtain a final estimate of our model's performance. We need to rescale the test set's images based on the range from our training images. It was 0 to 255, as you may remember.\n",
    "\n",
    "After rescaling, we evaluate using the [`evaluate`](https://tensorflow.rstudio.com/keras/reference/evaluate.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test <- data_test$x / 255\n",
    "y_test <- data_test$y\n",
    "model %>% evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the test set accuracy is comparable to the validation set accuracy. Although we could use the validation accuracy as a measure of how well we do on unseen the data, we still optimise our model on the validation set by adapting the hyperparameters ourselves. In this sense, the validation set is not truly unseen: we need to rely on the test set to give us a final verdict on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: early stopping\n",
    "Retrain the model from exercise 7. This time, set the number of epochs to the number of epochs at which the model started overfitting. Evaluate your model again.\n",
    "\n",
    "What happens to the accuracy and loss compared to the model you trained before? Why? Motivate your answer in a few sentences in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    layer_flatten(input_shape = c(28, 28)) %>%\n",
    "    <FILL IN>\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.001),\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    validation_data = list(x_val, y_val),\n",
    "    epochs = <FILL IN>,\n",
    "    batch_size = 1024,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: bonus\n",
    "Reduce the capacity of the model such that you do not overfit. How many layers and neurons do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: bonus\n",
    "Decrease the batch size by factors of four, and inspect the resulting loss curves. What happens when you approach a batch size of 1? How do you explain this behaviour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
