{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRUs improved\n",
    "In this notebook we will try to improve on our previous temperature prediction model using the Jena weather dataset. We will try to create a more realistic RNN model which treats the data as a single sequence. In addition to these improvments we will use regularisation techniques to reduce overfitting.\n",
    "\n",
    "In hopes to get the yearly, seasonal, patterns into our model, we will sample the data weekly and have a sequence length of a year and predict the temperature next week. If everything works out, we could effectively predict the same temperature as last year! By sub-sampling our data weekly, we make sure that our sequences are not too long but are still able to capture the seasonal changes. In order to have enough training data, we will only shift 1 datapoint forward in each iteration of sequence generation. \n",
    "\n",
    "The plan in this notebook is to;\n",
    "1. Load data and some cleaning.\n",
    "1. Sub-sample data for every week.\n",
    "1. Create sequences.\n",
    "1. Split data.\n",
    "1. Scale data.\n",
    "1. Baseline GRU RNN.\n",
    "1. Stateful GRU.\n",
    "1. Dropout applied to GRU.\n",
    "1. Train on all data (no validation) and evaluate\n",
    "\n",
    "We will provide more code in this notebook in order to get quicker to the modelling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(ggplot2)\n",
    "source(\"05-helpers.R\")\n",
    "\n",
    "use_multi_cpu(2L)\n",
    "# This specifies the number of threads TensorFlow will use. You can change this number, \n",
    "# but it depends on the model, what the optimal number of threads might be.\n",
    "# In my experiments 2 threads performed quite well for simpleRNNs.\n",
    "# Using more than 2 threads increased communication overhead and decreased the training speed.\n",
    "# To change this value simply set the value which you want and then click \"Kernel\" -> \"Shutdown\" and then \"Kernel\" -> \"Restart\".\n",
    "# If you do not shutdown the kernel, the change will not take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data <- load_jena_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the `Date Time` feature from `raw_data` and store the result in a variable called `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- raw_data[,-1]\n",
    "dim(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sub-sampling\n",
    "Let us now sub-sample our data by selecting a data point every week. Since our data has a measurement every 10 minutes we need to sample every 6 * 24 * 7 = 1008 data point. We use the same method as in the previous notebook to do this, the [`seq()`](https://www.rdocumentation.org/packages/base/versions/3.5.3/topics/seq) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(data)\n",
    "data_weekly = data[seq(1, nrow(data), 1008),]\n",
    "dim(data_weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not that many datapoints, but let us see if we can still see the seasonal pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data_weekly, aes(x = 1:nrow(data_weekly),y=`T (degC)`)) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating sequences\n",
    "Now we need to create sequences from our weekly data. To be sure that we get enough datapoints we will only shift a single data-point between sequences and predict the temperature next week. The function `create_sequences_x_y()` which we wrote in the last notebook is available for us to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Create the sequences we need by filling in the `create_sequences_x_y()` function and save them as `x_weekly` and `y_weekly`. Check your work by checking the dimensions of `x_weekly` and `y_weekly`. The dimensions for `x_weekly` should be `(365, 52, 14)`. The number 365 are not days, we just happen to have that many data examples.\n",
    "\n",
    "**Hint** Remember that the function takes 4 parameters; `data`, `sequence_length`, `target_shift` and `step_shift` and returns a list which contains `x` and `y`. If you are unable to find the correct values of these parameters, ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out <- create_sequences_x_y(data = ?, sequence_length = ?, target_shift = ?, step_shift = ?)\n",
    "x_weekly <- ?\n",
    "y_weekly <- ?\n",
    "dim(x_weekly)\n",
    "dim(y_weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we eliminate the other features from `y_weekly` and only keep the temperature and store the result in `y_weekly_temp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_weekly_temp <- y_weekly[,2]\n",
    "length(y_weekly_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting\n",
    "Now we split our data to train/test. Like in the previous notebook we use the last 20% of the data as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data <- split_dataset(x_data = x_weekly, y_data = y_weekly_temp, fraction = 0.2)\n",
    "x_train = split_data$x_train\n",
    "y_train = split_data$y_train\n",
    "x_test = split_data$x_test\n",
    "y_test = split_data$y_test\n",
    "dim(x_train)\n",
    "length(y_train)\n",
    "dim(x_test)\n",
    "length(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling\n",
    "The last preprocessing step, the scaling. This is exactly like in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the mean and sd for each feature independently, The features are on the 3rd axis.\n",
    "mean <- apply(x_train, 3, mean)\n",
    "std <- apply(x_train, 3, sd)\n",
    "# Create an empty 3D tensor with same dimensions as feature_matrix_reshaped.\n",
    "x_train_scaled <- array(0, dim = dim(x_train))\n",
    "x_test_scaled <- array(0, dim = dim(x_test))\n",
    "\n",
    "# For each feature we subtract the mean and divide by the std.\n",
    "for (j in 1:dim(x_train)[3]) {\n",
    "    x_train_scaled[,,j] <- (x_train[,,j] - mean[j]) / std[j]\n",
    "    x_test_scaled[,,j] <- (x_test[,,j] - mean[j]) / std[j]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The models\n",
    "Now we will start with a GRU and try to improve it.\n",
    "\n",
    "We have copied the network from the pervious notebook to the cell below but made some adjustments according to the changed problem.\n",
    "- We added more units (32) to the GRU layer.\n",
    "- We changed the input shape to account for a different sequence length.\n",
    "- We use a batch size of 1. This is to compare to the next exercise in which we create a \"stateful\" GRU.\n",
    "\n",
    "Making the batch size 1, will increase the training time considerably. While the model is training we suggest continue reading forward.\n",
    "\n",
    "**Hint** To speed up training it is helpful to remove the `callbacks=list(Progress$new())` line from the `fit` function. The side-effect is that you have less idea of how far in the training process you are (you still the see the `*` on the left or the hourglass at the top), but you can be sure that it is about 1/5 faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    layer_gru(units = 32, input_shape = c(52, 14)) %>%\n",
    "    layer_dense(units = 1)\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.001),\n",
    "    loss = \"mae\"\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = x_train_scaled,\n",
    "    y = y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 40,\n",
    "    batch_size = 1,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see an average validation loss of about 5 and the model is slighly overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful GRU\n",
    "Let us now make this model stateful. When we make an RNN stateful, we pass the state, or the \"memory\", of the previous batch as an initial state to the next batch. This effectivly allows the model to treat the whole dataset as a single sequence, as we can pass information from the first time-step all the way to the last time-step. Keep in mind that the model only trains (updates the weight of the model) based on the data which is in the current sequence. For many use-case a stateful RNN is what many people look for in \"production\".\n",
    "\n",
    "## Exercise 6.1\n",
    "Copy the previous model to the cell below and set, `stateful = TRUE` in the [`layer_gru()`](https://keras.rstudio.com/reference/layer_gru.html) function. When making an RNN stateful we also need to provide `batch_input_shape = c(1, 52, 14)` into the `layer_gru()` function. Additinally, we need to make sure that Keras **DOES NOT SHUFFLE** our training examples between epochs, that is the default behavior. To change this, set `shuffle = FALSE` in the `fit()` function. If Keras were to shuffle our examples, the state which gets passed between batches would become meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our model starts overfitting the data even more when making it stateful. That's great! Of course, in the sense that it is good that making the model stateful allowed the model to train even more, meaning that the model benefits from the state being passed forward.\n",
    "\n",
    "Let us try to fight this overfitting using some dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "Copy the previous model to the cell below and set some `recurrent_dropout` in the stateful [`layer_gru()`](https://keras.rstudio.com/reference/layer_gru.html). I found that a dropout value of 0.8 worked alright.\n",
    "\n",
    "**Hint** To speed up training it is helpful to remove the `callbacks=list(Progress$new())` line from the `fit` function. The side-effect is that you have less idea of how far in the training process you are, but you can be sure that it is about 1/5 faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are happy with the model and your hyperparameters we will train it one last time on all of the data and then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.3\n",
    "Copy the model from above and remove the `validation_split = 0.2` from the `fit()` function. This makes sure that we train all the data and the initial state to the test set is the last state, from the last sequence, from the training. Then run the cell below to evalute your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% evaluate(x_test_scaled, y_test, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also plot the actual temperature and the predicted temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test <- data.frame(y_test)\n",
    "ggplot(df_test, aes(x = 1:nrow(df_test),y=`y_test`)) + geom_line()\n",
    "\n",
    "predictions <- model %>% predict(x_test_scaled, batch_size = 1)\n",
    "df_predictions <- data.frame(predictions)\n",
    "ggplot(df_predictions, aes(x = 1:nrow(df_predictions),y=`predictions`)) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! It does not look to be directly copying the predictions of last year over and still looks something like our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we worked on an improved version of the temperature prediction problem.\n",
    "\n",
    "- We sampled our data weekly and predicted the temperature next week. \n",
    "- We made sure that our sequences were long enough to process a years worth of data.\n",
    "- We further improved our model so that it was stateful, storing and passing forward the state of the network between batches.\n",
    "- We then applied recurrent dropout to regularise our model to reduce overfitting. \n",
    "- Lastly, we trained on all the training set before finally evaluating our model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise\n",
    "Copy the model from above and try stacking 2 layers of GRUs. Add the `validation_split = 0.2` again. To stack RNNs we need to make sure that the first layer has the parameter `return_sequences = TRUE` but make sure that the later RNN does not. Be careful, stacking RNNs really kills your performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
