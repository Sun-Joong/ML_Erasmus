{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fashion MNIST - transfer learning\n",
    "This is the last time we will see the **fashion MNIST dataset**. This time we will use a pretrained CNN to process the images. For every image we want to classify it into **10 distinct clothing categories**. We will use the first layers from a known CNN, **ResNet50** and only use 10% of the dataset to speed up processing.\n",
    "\n",
    "The **ResNet50** expects the input to have **3 colors**, but the Fashion MNIST dataset is only grayscale. We will \"fix\" this issue by copying the grayscale values as the colors Red, Green, Blue. These images will undboutably look strange, but this allows us to use the ResNet50 model. Furthermore, the minimal image input size is **32 by 32** pixels, and our images are 28 by 28. To fix this, we will resize our images. Thus, our training set will have dimensions `(6000, 32, 32, 1)`. Lastly, we will do our normal **Min/Max scaling** and map the labels to their **one-hot encoding representation**. \n",
    "\n",
    "We will then download the ResNet50 model and chop off the last layers. We will then construct a new model which uses the first layers of the ResNet50 as the input layer and **freeze** the weights, so we do not train them. After feeding the input to the ResNet50 we will use the computed representation as input to a **dense layer** and then to a **softmax layer**. This will allow us to output a 10 class probability distribution specific to our problem and train the CNN using **categorical_crossentropy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data set and inspect the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>60000</li>\n",
       "\t<li>28</li>\n",
       "\t<li>28</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 60000\n",
       "\\item 28\n",
       "\\item 28\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 60000\n",
       "2. 28\n",
       "3. 28\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 60000    28    28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "60000"
      ],
      "text/latex": [
       "60000"
      ],
      "text/markdown": [
       "60000"
      ],
      "text/plain": [
       "[1] 60000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>10000</li>\n",
       "\t<li>28</li>\n",
       "\t<li>28</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 10000\n",
       "\\item 28\n",
       "\\item 28\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 10000\n",
       "2. 28\n",
       "3. 28\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 10000    28    28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "10000"
      ],
      "text/latex": [
       "10000"
      ],
      "text/markdown": [
       "10000"
      ],
      "text/plain": [
       "[1] 10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tensorflow)\n",
    "library(keras)\n",
    "library(abind)\n",
    "source(\"06-helpers.R\")\n",
    "\n",
    "use_multi_cpu(4L)\n",
    "\n",
    "data <- dataset_fashion_mnist()\n",
    "dim(data$train$x)\n",
    "length(data$train$y)\n",
    "dim(data$test$x)\n",
    "length(data$test$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reshape and subsampling\n",
    "\n",
    "The network we will be using accepts input shapes of (32, 32, 3). To rescale the FMNIST data to this shape, we first create a placeholder array of zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_resized <- array(0, dim = c(60000, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we repeat the FMNIST grayscale values three time to create RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train <- data$train$x\n",
    "x_train <- array_reshape(x_train, dim=c(60000 * 784))\n",
    "x_train <- cbind(x_train, x_train, x_train)\n",
    "x_train = array_reshape(x_train, dim=c(60000, 28, 28, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fill the FMNIST values in the placeholder array. Now, we essentially created a zero-padding around the FMNIST images such that they are (32, 32, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_resized[,3:30,3:30,] = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to do the same for the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:1: unexpected '<'\n1: <\n    ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:1: unexpected '<'\n1: <\n    ^\nTraceback:\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_resized <- array(0, dim = c(10000, 32, 32, 3))\n",
    "\n",
    "x_test <- data$test$x\n",
    "x_test <- array_reshape(x_test, dim=c(10000 * 784))\n",
    "x_test <- cbind(x_test, x_test, x_test)\n",
    "x_test = array_reshape(x_test, dim=c(10000, 28, 28, 3))\n",
    "\n",
    "x_test_resized[,3:30,3:30,] = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 One-hot encoding\n",
    "Lastly, the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y = data$train$y[1:60000], num_classes = 10)\n",
    "y_test = to_categorical(y = data$test$y[1:10000], num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ResNet50\n",
    "Now we want to use the first layers of the ResNet50 model, pretrained, as feature extractors for our final model. We start by downloading the model description along with the weights. \n",
    "\n",
    "The ResNet50 model which we download has been trained with a known dataset called [ImageNet](http://www.image-net.org/). This is a very large dataset (14 million images) thus we can benefitted from someone else spending a long time training on this dataset. The model was trained in the multiclass classification task over 1000 classes. We do not care to classify our clothing images as these classes so we specify `include_top = FALSE`, to indicate that we will not use the last layer of the model, which is the softmax layer over 1000 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time difference of 5.256554 secs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start <- Sys.time()\n",
    "\n",
    "k_clear_session()\n",
    "resnet_50_model <- application_resnet50(weights = 'imagenet', include_top = FALSE, input_shape = c(32, 32, 3))\n",
    "\n",
    "end <- Sys.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the layers on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)              Output Shape      Param #  Connected to               \n",
      "================================================================================\n",
      "input_1 (InputLayer)      [(None, 32, 32, 3 0                                   \n",
      "________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D) (None, 38, 38, 3) 0        input_1[0][0]              \n",
      "________________________________________________________________________________\n",
      "conv1 (Conv2D)            (None, 16, 16, 64 9472     conv1_pad[0][0]            \n",
      "________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalizat (None, 16, 16, 64 256      conv1[0][0]                \n",
      "________________________________________________________________________________\n",
      "activation (Activation)   (None, 16, 16, 64 0        bn_conv1[0][0]             \n",
      "________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D) (None, 18, 18, 64 0        activation[0][0]           \n",
      "________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling (None, 8, 8, 64)  0        pool1_pad[0][0]            \n",
      "________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)   (None, 8, 8, 64)  4160     max_pooling2d[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNorma (None, 8, 8, 64)  256      res2a_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_1 (Activation) (None, 8, 8, 64)  0        bn2a_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)   (None, 8, 8, 64)  36928    activation_1[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNorma (None, 8, 8, 64)  256      res2a_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_2 (Activation) (None, 8, 8, 64)  0        bn2a_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)   (None, 8, 8, 256) 16640    activation_2[0][0]         \n",
      "________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)    (None, 8, 8, 256) 16640    max_pooling2d[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNorma (None, 8, 8, 256) 1024     res2a_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormal (None, 8, 8, 256) 1024     res2a_branch1[0][0]        \n",
      "________________________________________________________________________________\n",
      "add (Add)                 (None, 8, 8, 256) 0        bn2a_branch2c[0][0]        \n",
      "                                                     bn2a_branch1[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_3 (Activation) (None, 8, 8, 256) 0        add[0][0]                  \n",
      "________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)   (None, 8, 8, 64)  16448    activation_3[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNorma (None, 8, 8, 64)  256      res2b_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_4 (Activation) (None, 8, 8, 64)  0        bn2b_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)   (None, 8, 8, 64)  36928    activation_4[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNorma (None, 8, 8, 64)  256      res2b_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_5 (Activation) (None, 8, 8, 64)  0        bn2b_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)   (None, 8, 8, 256) 16640    activation_5[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNorma (None, 8, 8, 256) 1024     res2b_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_1 (Add)               (None, 8, 8, 256) 0        bn2b_branch2c[0][0]        \n",
      "                                                     activation_3[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_6 (Activation) (None, 8, 8, 256) 0        add_1[0][0]                \n",
      "________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)   (None, 8, 8, 64)  16448    activation_6[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNorma (None, 8, 8, 64)  256      res2c_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_7 (Activation) (None, 8, 8, 64)  0        bn2c_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)   (None, 8, 8, 64)  36928    activation_7[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNorma (None, 8, 8, 64)  256      res2c_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_8 (Activation) (None, 8, 8, 64)  0        bn2c_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)   (None, 8, 8, 256) 16640    activation_8[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNorma (None, 8, 8, 256) 1024     res2c_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_2 (Add)               (None, 8, 8, 256) 0        bn2c_branch2c[0][0]        \n",
      "                                                     activation_6[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_9 (Activation) (None, 8, 8, 256) 0        add_2[0][0]                \n",
      "________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)   (None, 4, 4, 128) 32896    activation_9[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNorma (None, 4, 4, 128) 512      res3a_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_10 (Activation (None, 4, 4, 128) 0        bn3a_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)   (None, 4, 4, 128) 147584   activation_10[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNorma (None, 4, 4, 128) 512      res3a_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_11 (Activation (None, 4, 4, 128) 0        bn3a_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)   (None, 4, 4, 512) 66048    activation_11[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)    (None, 4, 4, 512) 131584   activation_9[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNorma (None, 4, 4, 512) 2048     res3a_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormal (None, 4, 4, 512) 2048     res3a_branch1[0][0]        \n",
      "________________________________________________________________________________\n",
      "add_3 (Add)               (None, 4, 4, 512) 0        bn3a_branch2c[0][0]        \n",
      "                                                     bn3a_branch1[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_12 (Activation (None, 4, 4, 512) 0        add_3[0][0]                \n",
      "________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)   (None, 4, 4, 128) 65664    activation_12[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNorma (None, 4, 4, 128) 512      res3b_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_13 (Activation (None, 4, 4, 128) 0        bn3b_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)   (None, 4, 4, 128) 147584   activation_13[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNorma (None, 4, 4, 128) 512      res3b_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_14 (Activation (None, 4, 4, 128) 0        bn3b_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)   (None, 4, 4, 512) 66048    activation_14[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNorma (None, 4, 4, 512) 2048     res3b_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_4 (Add)               (None, 4, 4, 512) 0        bn3b_branch2c[0][0]        \n",
      "                                                     activation_12[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_15 (Activation (None, 4, 4, 512) 0        add_4[0][0]                \n",
      "________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)   (None, 4, 4, 128) 65664    activation_15[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNorma (None, 4, 4, 128) 512      res3c_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_16 (Activation (None, 4, 4, 128) 0        bn3c_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)   (None, 4, 4, 128) 147584   activation_16[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNorma (None, 4, 4, 128) 512      res3c_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_17 (Activation (None, 4, 4, 128) 0        bn3c_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)   (None, 4, 4, 512) 66048    activation_17[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNorma (None, 4, 4, 512) 2048     res3c_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_5 (Add)               (None, 4, 4, 512) 0        bn3c_branch2c[0][0]        \n",
      "                                                     activation_15[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_18 (Activation (None, 4, 4, 512) 0        add_5[0][0]                \n",
      "________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)   (None, 4, 4, 128) 65664    activation_18[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNorma (None, 4, 4, 128) 512      res3d_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_19 (Activation (None, 4, 4, 128) 0        bn3d_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)   (None, 4, 4, 128) 147584   activation_19[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNorma (None, 4, 4, 128) 512      res3d_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_20 (Activation (None, 4, 4, 128) 0        bn3d_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)   (None, 4, 4, 512) 66048    activation_20[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNorma (None, 4, 4, 512) 2048     res3d_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_6 (Add)               (None, 4, 4, 512) 0        bn3d_branch2c[0][0]        \n",
      "                                                     activation_18[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_21 (Activation (None, 4, 4, 512) 0        add_6[0][0]                \n",
      "________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)   (None, 2, 2, 256) 131328   activation_21[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNorma (None, 2, 2, 256) 1024     res4a_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_22 (Activation (None, 2, 2, 256) 0        bn4a_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)   (None, 2, 2, 256) 590080   activation_22[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNorma (None, 2, 2, 256) 1024     res4a_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_23 (Activation (None, 2, 2, 256) 0        bn4a_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)   (None, 2, 2, 1024 263168   activation_23[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)    (None, 2, 2, 1024 525312   activation_21[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNorma (None, 2, 2, 1024 4096     res4a_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormal (None, 2, 2, 1024 4096     res4a_branch1[0][0]        \n",
      "________________________________________________________________________________\n",
      "add_7 (Add)               (None, 2, 2, 1024 0        bn4a_branch2c[0][0]        \n",
      "                                                     bn4a_branch1[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_24 (Activation (None, 2, 2, 1024 0        add_7[0][0]                \n",
      "________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)   (None, 2, 2, 256) 262400   activation_24[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNorma (None, 2, 2, 256) 1024     res4b_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_25 (Activation (None, 2, 2, 256) 0        bn4b_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)   (None, 2, 2, 256) 590080   activation_25[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNorma (None, 2, 2, 256) 1024     res4b_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_26 (Activation (None, 2, 2, 256) 0        bn4b_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)   (None, 2, 2, 1024 263168   activation_26[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNorma (None, 2, 2, 1024 4096     res4b_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_8 (Add)               (None, 2, 2, 1024 0        bn4b_branch2c[0][0]        \n",
      "                                                     activation_24[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_27 (Activation (None, 2, 2, 1024 0        add_8[0][0]                \n",
      "________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)   (None, 2, 2, 256) 262400   activation_27[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNorma (None, 2, 2, 256) 1024     res4c_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_28 (Activation (None, 2, 2, 256) 0        bn4c_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)   (None, 2, 2, 256) 590080   activation_28[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNorma (None, 2, 2, 256) 1024     res4c_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_29 (Activation (None, 2, 2, 256) 0        bn4c_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)   (None, 2, 2, 1024 263168   activation_29[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNorma (None, 2, 2, 1024 4096     res4c_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_9 (Add)               (None, 2, 2, 1024 0        bn4c_branch2c[0][0]        \n",
      "                                                     activation_27[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_30 (Activation (None, 2, 2, 1024 0        add_9[0][0]                \n",
      "________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)   (None, 2, 2, 256) 262400   activation_30[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNorma (None, 2, 2, 256) 1024     res4d_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_31 (Activation (None, 2, 2, 256) 0        bn4d_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)   (None, 2, 2, 256) 590080   activation_31[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNorma (None, 2, 2, 256) 1024     res4d_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_32 (Activation (None, 2, 2, 256) 0        bn4d_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)   (None, 2, 2, 1024 263168   activation_32[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNorma (None, 2, 2, 1024 4096     res4d_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_10 (Add)              (None, 2, 2, 1024 0        bn4d_branch2c[0][0]        \n",
      "                                                     activation_30[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_33 (Activation (None, 2, 2, 1024 0        add_10[0][0]               \n",
      "________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)   (None, 2, 2, 256) 262400   activation_33[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNorma (None, 2, 2, 256) 1024     res4e_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_34 (Activation (None, 2, 2, 256) 0        bn4e_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)   (None, 2, 2, 256) 590080   activation_34[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNorma (None, 2, 2, 256) 1024     res4e_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_35 (Activation (None, 2, 2, 256) 0        bn4e_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)   (None, 2, 2, 1024 263168   activation_35[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNorma (None, 2, 2, 1024 4096     res4e_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_11 (Add)              (None, 2, 2, 1024 0        bn4e_branch2c[0][0]        \n",
      "                                                     activation_33[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_36 (Activation (None, 2, 2, 1024 0        add_11[0][0]               \n",
      "________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)   (None, 2, 2, 256) 262400   activation_36[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNorma (None, 2, 2, 256) 1024     res4f_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_37 (Activation (None, 2, 2, 256) 0        bn4f_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)   (None, 2, 2, 256) 590080   activation_37[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNorma (None, 2, 2, 256) 1024     res4f_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_38 (Activation (None, 2, 2, 256) 0        bn4f_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)   (None, 2, 2, 1024 263168   activation_38[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNorma (None, 2, 2, 1024 4096     res4f_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_12 (Add)              (None, 2, 2, 1024 0        bn4f_branch2c[0][0]        \n",
      "                                                     activation_36[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_39 (Activation (None, 2, 2, 1024 0        add_12[0][0]               \n",
      "________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)   (None, 1, 1, 512) 524800   activation_39[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNorma (None, 1, 1, 512) 2048     res5a_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_40 (Activation (None, 1, 1, 512) 0        bn5a_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)   (None, 1, 1, 512) 2359808  activation_40[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNorma (None, 1, 1, 512) 2048     res5a_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_41 (Activation (None, 1, 1, 512) 0        bn5a_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)   (None, 1, 1, 2048 1050624  activation_41[0][0]        \n",
      "________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)    (None, 1, 1, 2048 2099200  activation_39[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNorma (None, 1, 1, 2048 8192     res5a_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormal (None, 1, 1, 2048 8192     res5a_branch1[0][0]        \n",
      "________________________________________________________________________________\n",
      "add_13 (Add)              (None, 1, 1, 2048 0        bn5a_branch2c[0][0]        \n",
      "                                                     bn5a_branch1[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_42 (Activation (None, 1, 1, 2048 0        add_13[0][0]               \n",
      "________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)   (None, 1, 1, 512) 1049088  activation_42[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNorma (None, 1, 1, 512) 2048     res5b_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_43 (Activation (None, 1, 1, 512) 0        bn5b_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)   (None, 1, 1, 512) 2359808  activation_43[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNorma (None, 1, 1, 512) 2048     res5b_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_44 (Activation (None, 1, 1, 512) 0        bn5b_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)   (None, 1, 1, 2048 1050624  activation_44[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNorma (None, 1, 1, 2048 8192     res5b_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_14 (Add)              (None, 1, 1, 2048 0        bn5b_branch2c[0][0]        \n",
      "                                                     activation_42[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_45 (Activation (None, 1, 1, 2048 0        add_14[0][0]               \n",
      "________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)   (None, 1, 1, 512) 1049088  activation_45[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNorma (None, 1, 1, 512) 2048     res5c_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_46 (Activation (None, 1, 1, 512) 0        bn5c_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)   (None, 1, 1, 512) 2359808  activation_46[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNorma (None, 1, 1, 512) 2048     res5c_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_47 (Activation (None, 1, 1, 512) 0        bn5c_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)   (None, 1, 1, 2048 1050624  activation_47[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNorma (None, 1, 1, 2048 8192     res5c_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_15 (Add)              (None, 1, 1, 2048 0        bn5c_branch2c[0][0]        \n",
      "                                                     activation_45[0][0]        \n",
      "________________________________________________________________________________\n",
      "activation_48 (Activation (None, 1, 1, 2048 0        add_15[0][0]               \n",
      "================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "summary(resnet_50_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This is a very big model, 23 million parameters with lots of layers. In fact, processing our images through this network takes some time and we also expect the first features learned by the network to be beneficial to our problem so in the next cell we only use the layers up to the layer `activation_6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_resnet_50 <- keras_model(inputs = resnet_50_model$input, \n",
    "                                 outputs = get_layer(resnet_50_model, 'activation_6')$output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we do not want to train those layers. Let's make sure that we do not train all of these layers by freezing the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)              Output Shape      Param #  Connected to               \n",
      "================================================================================\n",
      "input_1 (InputLayer)      [(None, 32, 32, 3 0                                   \n",
      "________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D) (None, 38, 38, 3) 0        input_1[0][0]              \n",
      "________________________________________________________________________________\n",
      "conv1 (Conv2D)            (None, 16, 16, 64 9472     conv1_pad[0][0]            \n",
      "________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalizat (None, 16, 16, 64 256      conv1[0][0]                \n",
      "________________________________________________________________________________\n",
      "activation (Activation)   (None, 16, 16, 64 0        bn_conv1[0][0]             \n",
      "________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D) (None, 18, 18, 64 0        activation[0][0]           \n",
      "________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling (None, 8, 8, 64)  0        pool1_pad[0][0]            \n",
      "________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)   (None, 8, 8, 64)  4160     max_pooling2d[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNorma (None, 8, 8, 64)  256      res2a_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_1 (Activation) (None, 8, 8, 64)  0        bn2a_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)   (None, 8, 8, 64)  36928    activation_1[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNorma (None, 8, 8, 64)  256      res2a_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_2 (Activation) (None, 8, 8, 64)  0        bn2a_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)   (None, 8, 8, 256) 16640    activation_2[0][0]         \n",
      "________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)    (None, 8, 8, 256) 16640    max_pooling2d[0][0]        \n",
      "________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNorma (None, 8, 8, 256) 1024     res2a_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormal (None, 8, 8, 256) 1024     res2a_branch1[0][0]        \n",
      "________________________________________________________________________________\n",
      "add (Add)                 (None, 8, 8, 256) 0        bn2a_branch2c[0][0]        \n",
      "                                                     bn2a_branch1[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_3 (Activation) (None, 8, 8, 256) 0        add[0][0]                  \n",
      "________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)   (None, 8, 8, 64)  16448    activation_3[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNorma (None, 8, 8, 64)  256      res2b_branch2a[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_4 (Activation) (None, 8, 8, 64)  0        bn2b_branch2a[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)   (None, 8, 8, 64)  36928    activation_4[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNorma (None, 8, 8, 64)  256      res2b_branch2b[0][0]       \n",
      "________________________________________________________________________________\n",
      "activation_5 (Activation) (None, 8, 8, 64)  0        bn2b_branch2b[0][0]        \n",
      "________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)   (None, 8, 8, 256) 16640    activation_5[0][0]         \n",
      "________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNorma (None, 8, 8, 256) 1024     res2b_branch2c[0][0]       \n",
      "________________________________________________________________________________\n",
      "add_1 (Add)               (None, 8, 8, 256) 0        bn2b_branch2c[0][0]        \n",
      "                                                     activation_3[0][0]         \n",
      "________________________________________________________________________________\n",
      "activation_6 (Activation) (None, 8, 8, 256) 0        add_1[0][0]                \n",
      "================================================================================\n",
      "Total params: 158,208\n",
      "Trainable params: 0\n",
      "Non-trainable params: 158,208\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "freeze_weights(smaller_resnet_50)\n",
    "summary(smaller_resnet_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there are no trainable parameters in this model. Good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Our model\n",
    "Let us add a dense layer and our own softmax layer to the `smaller_resnet_50` model and start training it.\n",
    "\n",
    "Take notice of\n",
    "- The first \"layer\" in our network is the `smaller_resnet_50` model. It takes the initial input and outputs some features.\n",
    "- The small learning rate. A small learning rate is very common when training with pretrained networks.\n",
    "- Long training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train <- imagenet_preprocess_input(x_train_resized, mode='tf')\n",
    "x_test <- imagenet_preprocess_input(x_test_resized, mode='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "model (Model)                       (None, 8, 8, 256)               158208      \n",
      "________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNormali (None, 8, 8, 256)               1024        \n",
      "________________________________________________________________________________\n",
      "flatten_3 (Flatten)                 (None, 16384)                   0           \n",
      "________________________________________________________________________________\n",
      "dense1 (Dense)                      (None, 10)                      163850      \n",
      "================================================================================\n",
      "Total params: 323,082\n",
      "Trainable params: 164,362\n",
      "Non-trainable params: 158,720\n",
      "________________________________________________________________________________\n",
      "Epoch 5 - loss: 0.2071848, acc.: 0.9322292, val loss: 2.62665552393595, val acc.: 0.637916684150696  \r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAABmJLR0QA/wD/AP+gvaeTAAAg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\n2AEAACji/wBB9o8gQzfwEAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    smaller_resnet_50() %>%\n",
    "    layer_batch_normalization() %>%\n",
    "    layer_flatten() %>%\n",
    "    layer_dense(units = 10, activation = \"softmax\", name = 'dense1')\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.001),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metric = \"acc\"\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs = 5,\n",
    "    batch_size = 512,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that our model is fairly expressive and we could maybe continue to train it and seen some performance improvements but we leave this is a bonus exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$loss</dt>\n",
       "\t\t<dd>2.59368507003784</dd>\n",
       "\t<dt>$acc</dt>\n",
       "\t\t<dd>0.645099997520447</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$loss] 2.59368507003784\n",
       "\\item[\\$acc] 0.645099997520447\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$loss\n",
       ":   2.59368507003784\n",
       "$acc\n",
       ":   0.645099997520447\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$loss\n",
       "[1] 2.593685\n",
       "\n",
       "$acc\n",
       "[1] 0.6451\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model %>% evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we saw how to use a pretrained CNN and how we could add our layers to the model so that we can benefit from the generic features learned by the model. Some notes about this notebook.\n",
    "- Our images are only grayscale, but the model assumed colors. We can expect this to have some negative (or at least wasted computation) on our performance.\n",
    "- We needed to resize our images in order for them to fit. We can also expect this to have negative impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Excercise\n",
    "Try improving the model above. Consider the options below\n",
    "- Adding more layers from the ResNet50 model\n",
    "- Adding dropout.\n",
    "- Adding more dense layers/more dense units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_resnet_50 <- keras_model(inputs = resnet_50_model$input, \n",
    "                                 outputs = get_layer(resnet_50_model, 'activation_48')$output)\n",
    "freeze_weights(smaller_resnet_50)\n",
    "model <- keras_model_sequential() %>%\n",
    "    smaller_resnet_50 %>%\n",
    "    layer_dropout(0.2) %>%\n",
    "    layer_flatten() %>%\n",
    "    layer_dense(units = 32, activation = \"relu\") %>%\n",
    "    layer_dropout(0.2) %>%\n",
    "    layer_dense(units = 10, activation = \"softmax\")\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.0001),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metric = \"acc\"\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 20,\n",
    "    batch_size = 128,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
