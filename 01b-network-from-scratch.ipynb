{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A neural network from scratch\n",
    "The problem you solved in the last notebook was linearly separable. In this notebook, we will look at a famous problem that is not linearly separable: the **XOR problem**. We cannot solve it with a single neuron, but need a network of three neurons, of which two of them are in th network's *hidden layer*.\n",
    "\n",
    "Let's load the data set first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"01-helpers.R\")\n",
    "data <- dataset_xor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Extract the input matrix `X` and the labels `y` from the `data` object. Inspect them and their dimensions. Do they make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a hidden layer with two neurons to solve this problem. The entire network will look like this:\n",
    "\n",
    "![XOR problem network](images/hidden-layer-neurons.png)\n",
    "\n",
    "This may look complicated, but, like the network in the first notebook, we will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, you implemented a `neuron` function in the last notebook, looking like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid <- function(x) {\n",
    "    1 / (1 + exp(-x))\n",
    "}\n",
    "\n",
    "neuron <- function(X) {\n",
    "    sigmoid(w %*% t(X) + b)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights in this function are given as a vector `w`. In this way, each point $(x_1, x_2)$ is converted to the output of a single neuron. Since we have two neurons, the output of the multiplication step should be **two** numbers. We accomplish this by taking the weights as a matrix `W`, instead of as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Create the weight matrix `W` such that $\\mathbf{Z} = \\mathbf{W} \\mathbf{X}^\\top $ will look as follows: \n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{W} \\mathbf{X}^\\top =\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "0 & 1 & 1 & 2 \\\\\n",
    "0 & 1 & 1 & 2 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Save the output of the multiplication to the variable `Z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "We will need to add a bias term to each output variable. Since we have two output variables (two columns in `Z`), our bias variable `b` will be a vector instead of a single number like before. Create a bias vector `b` that looks as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "-1 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `apply` function\n",
    "We want to apply this bias to each row in `Z`. We will use R's [`apply`](https://www.rdocumentation.org/packages/base/versions/3.5.2/topics/apply) function to do so. As the name suggests, `apply` will apply a function to each element along a certain axis of a tensor. You will supply the input tensor, the axis along which to apply the operation, and the function to apply to it.\n",
    "\n",
    "Let's look at an example where we solve the addition of the bias term to each column in `Z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z <- apply(Z, 2, function(column) column + b)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `apply` takes as its input the following parameters:\n",
    "1. The tensor to which to apply a function\n",
    "1. The axis along which to apply the function - `1` for rows, `2` for columns, etc.\n",
    "1. The function to apply to each element\n",
    "\n",
    "By specifying `2` as the second argument, `apply` will loop to all columns and pass each column to the function you have defined. The function will simply add the bias term `b` to the column and return it.\n",
    "\n",
    "You will need to use `apply` quite often to transform a tensor in a certain way. You will have some time to practice with it and become familiar with it in the notebooks and the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the transformation that the layer applies to the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(t(Z), y)  # We need to transpose Z since plot_dataset expects each instance to be a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the problem is still not linearly separable: all instances lie along a line with slope 1, and we cannot draw a straight line across our plot to separate one class from the other. For that, we will need an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Implement the ReLU activation function introduced in the slides and apply it to each row in `Z` with the `apply` function. The ReLU function is given as\n",
    "\n",
    "$$\n",
    "\\sigma(x) = max(x, 0)\n",
    "$$\n",
    "\n",
    "Store the result in a variable called `A`. (`A` for the activation of the hidden layer), like so:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = relu(\\mathbf{Z})\n",
    "$$\n",
    "\n",
    "**Hint**: you cannot use R's `max`, because it will return the maximum across the entire vector. We need an *element-wise* operation, which you can find on the following documentation page: [`Extremes`](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/Extremes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu <- function(x) {\n",
    "    <FILL IN>\n",
    "}\n",
    "\n",
    "A <- <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Plot `A` together with `y` using the `plot_dataset` helper function. What is the effect of the activation function? How will the activation function allow us to solve this non-linear problem?\n",
    "\n",
    "**Hint**: remember that `plot_dataset` expects each instance (point) as a row, and not a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're nearly there! We will be looking at probabilities and decision boundaries and We have the result of the **hidden layer** in the variable `A`, and need to transform it into probabilities for each point in our input data, that is: a vector of length 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Create a vector `w` that will transform `A` into a vector of length 4. Set all elements of `w` to `1` for now, so that the following equation holds:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w} \\mathbf{A} = \\left(\n",
    "\\begin{array}{c}\n",
    "0 & 1 & 1 & 3 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these outputs are not probabilities, since we have a `3` as the last element in the output. Typically we want to have a sigmoid activation function or something similar to get the output between 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "In this case, we are skipping this step. Instead, you will modify `w` yourself such that we solve the problem perfectly. You can plot the resulting decision boundary with your choice of `w` by creating a function that returns the output of the entire network, and feeding it into the `plot_predictions` function.\n",
    "\n",
    "Create a function called `network` that will return the output of the network as a vector of length 4, by filling in the skeleton below. Modify the values of `w` such that your model classifies all points correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network <- function(X) {\n",
    "    Z <- <FILL IN>\n",
    "    A <- <FILL IN>\n",
    "    w <- <FILL IN>\n",
    "    w %*% A\n",
    "}\n",
    "\n",
    "plot_predictions(X, y, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: bonus\n",
    "Solve the problem again with the network changed as follows:\n",
    "1. Instead of a ReLU function in the hidden layer, use a sigmoid function\n",
    "1. Add a bias term to the output layer called `b_o`\n",
    "1. Add sigmoid activation to the output layer\n",
    "\n",
    "**Hints**:\n",
    "1. We cannot get perfect classifications (0 and 1) in this exercise. Instead, we think of the output as probabilities and **threshold** them: any $y < 0.5$ = class 0 and any $y \\geq 0.5$ = class 1.\n",
    "1. you can use `plot_dataset` to plot the results of the hidden layer like in exercise 5. Finding a decision boundary in this representation space is equivalent to finding the weights and bias of the output layer.\n",
    "\n",
    "For reference, you can try implementing the network from the following diagram:\n",
    "![A complete, sigmoid-only network](images/full-exercise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
