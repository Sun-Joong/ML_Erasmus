{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The XOR problem revisited with Keras\n",
    "In the last session you solved the XOR problem by hand. This session you will work on a modified XOR problem and solve it with Keras.\n",
    "\n",
    "Let's load Keras first, as well as some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "source(\"02-helpers.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- dataset_extended_xor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Extract the input matrix `X` and the labels `y` from the `data` object and assign them to the variables `X` and `y`.\n",
    "Plot the data set using the `plot_dataset` function, by providing it with the `X` and `y` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- data$X\n",
    "y <- data$y\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple Keras model with one hidden layer of four neurons and a ReLU activation function, and an output layer with one neuron with sigmoid activation. Running this cell will show a summary of the model with the different layers.\n",
    "\n",
    "**Note**: the `%>%` operator will apply the output of the left-hand side as the **first parameter** of the right-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    # The input_shape parameter in the line below refers to the number of input features (x1 and x2)\n",
    "    layer_dense(units = 4, input_shape = 2, activation = \"relu\") %>%\n",
    "    layer_dense(units = 1, activation = \"sigmoid\")\n",
    "\n",
    "model  # this will show a summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shape of each layer always has the batch size of the layer as its first dimension. Because the batch size is not known, this first dimension is reported as `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Why do we have an output shape of `(None, 4)` in the first layer, and `(None, 1)` in the second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None is the batch size. 4 is the number of neurons in the hidden layer, 1 in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Why do we have 12 parameters in the hidden layer, and 5 in the output layer?\n",
    "\n",
    "**Hint**: it may help to draw the neurons and weights on a piece of paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer (12 parameters): 4 neurons, each with 2 inputs and one bias = (4 weight + 1 bias parameters) x 3\n",
    "# Output layer (5 parameters): 1 neuron with 4 inputs and one bias = 4 weight + 1 bias parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to **compile** the model before training it to configure the learning process. We do that by specifying three parameters:\n",
    "1. The optimizer - Adam, with learning rate 0.01\n",
    "1. A loss function - binary cross-entropy (more on this later), since this is a two-class problem\n",
    "1. A metric - accuracy, since this is a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.01),\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model by using the [`fit`](https://keras.rstudio.com/reference/fit.html) function. We feed it:\n",
    "1. The input data `X`\n",
    "1. The labels `y`\n",
    "1. The number of epochs\n",
    "1. The batch size\n",
    "1. A list of callbacks. We have created a single callback for you that provides some information on the training progress.\n",
    "\n",
    "More information, such as the loss and accuracy for each epoch, is stored in the `history` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model %>% fit(\n",
    "    x = X,\n",
    "    y = as.numeric(y) - 1, # the labels are factors, and need to be converted to an ordinary vector of zeroes and ones\n",
    "    epochs = 100,\n",
    "    batch_size = 16,\n",
    "    callbacks=list(Progress$new())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the loss decrease during training, and converge after 30 or 40 epochs. The accuracy will increase, though not necessarily in proportion to the decrease in loss.\n",
    "\n",
    "Let's plot the predictions that this model makes on the XOR data set with the `plot_predictions_keras` function we have made for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_keras(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will do reasonably well to very well, depending on how 'lucky' you were during training. The decision boundary will follow the contours of the distribution, but also miss some outliers.\n",
    "\n",
    "Because there is a large degree of randomness involved in training a deep learning model, the performance of the model may vary considerably. This is exacerbated by the fact that we have a small data set, and a relatively large learning rate. In our test runs, accuracy at epoch 100 varied between 0.7 and 0.95, so your mileage may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "The goal of this exercise is to get the accuracy higher than 0.95. To do so, vary the following parameters:\n",
    "\n",
    "1. The number of units in the hidden layer (you can try dozens, thousands, etc.)\n",
    "1. The learning rate. Increase or decrease it by factors of 10 (e.g. 0.01, 0.001, 0.1, etc.)\n",
    "\n",
    "We have provided you with a skeleton that you can fill out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    layer_dense(units = 512, input_shape = 2, activation = \"relu\") %>%\n",
    "    layer_dense(units = 1, activation = \"sigmoid\")\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.01),\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = X,\n",
    "    y = as.numeric(y) - 1, # the labels are factors, and need to be converted to an ordinary vector of zeroes and ones\n",
    "    epochs = 500,\n",
    "    batch_size = 16,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "plot(history)\n",
    "plot_predictions_keras(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Add more hidden layers to your network, and try to classify the data set perfectly. That is: accuracy is equal to 1.\n",
    "\n",
    "**Hints**:\n",
    "1. Be careful adding too many layers, as the number of trainable parameters increases very quickly\n",
    "1. You may need to modify the learning rate when adding more layers\n",
    "1. As in exercise 4, increasing the number of epochs may help to increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    layer_dense(units = 512, input_shape = 2, activation = \"relu\") %>%\n",
    "    layer_dense(units = 512, input_shape = 2, activation = \"relu\") %>%\n",
    "    layer_dense(units = 512, input_shape = 2, activation = \"relu\") %>%\n",
    "    layer_dense(units = 1, activation = \"sigmoid\")\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.001),\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = X,\n",
    "    y = as.numeric(y) - 1, # the labels are factors, and need to be converted to an ordinary vector of zeroes and ones\n",
    "    epochs = 1000,\n",
    "    batch_size = 16,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "plot(history)\n",
    "plot_predictions_keras(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Do you think this is a reasonable model? Why?\n",
    "\n",
    "**Hint**: do you think this model will generalise well to more samples drawn from the same distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data set fits perfectly, your decision boundary will not be smooth, but have small islands and protrusions\n",
    "# to fit perfectly to outliers. These will be specific to your current data set, but not to a similar data set taken \n",
    "# from the same distribution. The network has become overly specific to this particular dataset (it has memorised it\n",
    "# perfectly), and will have reduced accuracy on a slightly different one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: bonus\n",
    "We have prepared another artificial data set for you, looking like a noisy checkerboard. It is slightly more complicated than the modified XOR problem of the previous exercises.\n",
    "\n",
    "Try to solve the problem in a way that generalises well. That is: when presented with new samples from the same problem, the accuracy of the model will be comparable. Validate your trained model with the dataset returned by the function `dataset_checkerboard_validation`.\n",
    "\n",
    "**Hint**: you can try increasing the batch size to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- dataset_checkerboard()\n",
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- data$X\n",
    "y <- data$y\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    layer_dense(units = 64, input_shape = 2, activation = \"relu\") %>%\n",
    "    layer_dense(units = 32, activation = \"relu\") %>%\n",
    "    layer_dense(units = 1, activation = \"sigmoid\")\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.01),\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = c(\"accuracy\")\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = X,\n",
    "    y = as.numeric(y) - 1, # the labels are factors, and need to be converted to an ordinary vector of zeroes and ones\n",
    "    epochs = 1000,\n",
    "    batch_size = 1024,\n",
    "    shuffle = TRUE,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "plot(history)\n",
    "plot_predictions_keras(X, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val <- dataset_checkerboard_validation()\n",
    "X_val <- data_val$X\n",
    "y_val <- data_val$y\n",
    "\n",
    "plot_dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_keras(X_val, y_val, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
