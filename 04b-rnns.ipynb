{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential data modelling\n",
    "We have added a `?` in the exercises where you need to fill in.\n",
    "\n",
    "In this notebook we will use RNNs to predict future temperature using the Jena weather dataset. We will start by using a SimpleRNN and then see whether we can improve our predictions using a GRU.\n",
    "\n",
    "Before the modelling step we need to do our usual preprocessing. This time you will see a few things you have not seen before.\n",
    "\n",
    "1. We will resample the data. The data consists of measurements every 10 minutes, we will instead select a datapoint every hour using the `seq()` function to select our data. We do this to make our training a little faster and have a somewhat meaningful problem.\n",
    "1. We will see a different way to create sequences from a dataset. This approach is more dynamic than the reshape approach as it can produce more sequences than the reshape approach. Hopefully, it is also more intuitive.\n",
    "1. We will not split our training data into a validation set, as we will leave that splitting up to Keras when we try to fit our data. Thus, we only split into train/test.\n",
    "\n",
    "In short;\n",
    "1. Load data and some cleaning.\n",
    "1. Sub-sample data for every hour.\n",
    "1. Create sequences.\n",
    "1. Split data.\n",
    "1. Scale data.\n",
    "1. Baseline SimpleRNN\n",
    "1. GRU RNN\n",
    "1. LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(ggplot2)\n",
    "source(\"04-helpers.R\")\n",
    "\n",
    "use_multi_cpu(2L)\n",
    "# This specifies the number of threads TensorFlow will use. You can change this number, \n",
    "# but it depends on the model, what the optimal number of threads might be.\n",
    "# In my experiments 2 threads performed quite well for simpleRNNs.\n",
    "# Using more than 2 threads increased communication overhead between CPUs and decreased the training speed.\n",
    "# To change this value simply set the value which you want and then -\n",
    "# click \"Kernel\" -> \"Shutdown\" and then \"Kernel\" -> \"Restart\".\n",
    "# If you do not shutdown the kernel, the change will not take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data <- load_jena_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1\n",
    "Remove the `Date Time` feature from `raw_data` and store the result in a variable called `data`. Verify your work using the `dim()` function. The data should have dimension `(420551, 14)`.\n",
    "\n",
    "**Hint** `Date Time` is the first column in `raw_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sub-sampling\n",
    "Let us now sub-sample our data by selecting every 6th data point. This will leave us with a data point for every hour instead of every 10 minutes.\n",
    "\n",
    "To do this we will use slicing in conjunction with the [`seq()`](https://www.rdocumentation.org/packages/base/versions/3.5.3/topics/seq) to specify the indices we want to select. To see how it works, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = c(1,2,3,4,5,6)\n",
    "odds = vec[seq(1, length(vec), 2)]\n",
    "odds\n",
    "evens = vec[seq(2, length(vec), 2)]\n",
    "evens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Select every 6th datapoint from the dataset using the `seq()` function. Save the resampled data in a variable called `data_hourly`. Verify your work using the `dim()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week we want to predict the temperature, `T (degC)`. Let us plot that feature and see how the temperature varies over those years using the resampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data_hourly, aes(x = 1:nrow(data_hourly),y=`T (degC)`)) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating sequences\n",
    "Now we will create sequences from the data using a function. The function accepts a number of parameters and returns a list containing two objects, the sequcences (`x`) and the targets (`y`).\n",
    "\n",
    "Let us briefly explain what the parameters do. First you specify the `data` (the function expects 2D tensor / matrix) which is used to generate the sequences. The `sequence_length` is the length of the sequence to generate, each sequence will have that length. The `target_shift` is the number of time-steps between the end of each sequence until the target, the time-step we want to predict. The `step_shift` is the number of time-steps between the start of each sequence. The function will try to use as many datapoints as possible given the parameters to create the sequences.\n",
    "\n",
    "Run the cell below to define the function and continue reading below. There is no need to read through the function and try to understand it. We just need to know how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sequences_x_y <- function(data, sequence_length, target_shift, step_shift) {\n",
    "    data <- as.matrix(data)\n",
    "    start_index = 1\n",
    "    end_index = dim(data)[1]\n",
    "    # We assume that the input is in legal ranges\n",
    "    elements = end_index - start_index + 1\n",
    "    # our targets are a single data point\n",
    "    target_len = 1\n",
    "    single_sequence_length = sequence_length + target_shift + target_len\n",
    "    number_of_sequences = floor((elements - single_sequence_length)/step_shift) + 1\n",
    "    \n",
    "    # Initialise variables we need in the loop\n",
    "    # We store the index which we should start with in each loop in current_start_index\n",
    "    current_start_index = start_index\n",
    "    sequence_x <- array(0, dim = c(number_of_sequences, sequence_length, dim(data)[2]))\n",
    "    sequence_y <- array(0, dim = c(number_of_sequences, dim(data)[2]))\n",
    "    for (sequence_index in 1:number_of_sequences) {\n",
    "        # We get the current sequence data\n",
    "        sequence_x[sequence_index,,] <- data[current_start_index:(current_start_index+sequence_length-1),]\n",
    "        sequence_y[sequence_index,] <- data[(current_start_index+sequence_length+target_shift):(current_start_index+sequence_length+target_shift+target_len-1),]\n",
    "        # We update our next start\n",
    "        current_start_index <- current_start_index + step_shift\n",
    "    }\n",
    "    list(x = sequence_x, y = sequence_y)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "To get a clearer idea of how this function works. Take the first 7 elements from `data_hourly` and pass it as `data` to the function. Set the `sequence_length` to 3, `target_shift` to 1 and `step_shift` to 1. The function returns a list which has two objects, `x` and `y`. Check the dimensions of `x` and `y` and see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_test_data <- create_sequences_x_y(data = ?, sequence_length = ?, target_shift = ?, step_shift = ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "Copy your solution to exercise 3.1 and try using the first 8 (instead of 7) elements of `data_hourly` as `data`. What are the dimensions now? Does it make sense that a new sequence was added? What if we increase the `step_shift` to 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3\n",
    "Now input all `data_hourly` as `data` and set the `sequence_length` to 96 (3 days), `target_shift` to 23 (23 hours) and `step_shift` to 24 (1 day). Store the `x` part of the output as `x_hourly` and `y` part as `y_hourly`. Check the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data <- create_sequences_x_y(data = ?, sequence_length = ?, target_shift = ?, step_shift = ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4\n",
    "Notice that `y_hourly`, our targets, have 14 features in them. Since we only want to predict the temperature, we should remove all other features from `y_hourly` and save it as `y_hourly_temp`. Put differently, only keep the temperature feature. Verify the dimensions as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hourly_temp <- ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting\n",
    "Now we split our data to train/test. Unlike usually, we will not split our data to a validation set. We will let Keras handle that for us in the `fit()` function.\n",
    "\n",
    "Like above, we have provided a function which splits the data for you. The function accepts three parameters `x_data` and `y_data`, the data it is supposed to split and a `fraction`, the fraction of the data that should be used as test data. The function will select the last elements from the to create the test set. The function assumes that the number of rows in `x_data` and `y_data` is the same.\n",
    "\n",
    "The function returns a list containing `x_train`, `y_train`, `x_test` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset <- function(x_data, y_data, fraction = 0.2) {\n",
    "    train_start_index <- 1\n",
    "    train_end_index <- train_start_index + floor((1-fraction) * dim(x_data)[1]) - 1\n",
    "    test_start_index <- train_end_index + 1\n",
    "    test_end_index <- dim(x_data)[1]\n",
    "    \n",
    "    x_train <- x_data[train_start_index:train_end_index,,]\n",
    "    x_test <- x_data[test_start_index:test_end_index,,]\n",
    "    \n",
    "    y_train <- y_data[train_start_index:train_end_index]\n",
    "    y_test <- y_data[test_start_index:test_end_index]\n",
    "    \n",
    "    list(x_train = x_train,\n",
    "        y_train = y_train,\n",
    "        x_test = x_test,\n",
    "        y_test = y_test)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 \n",
    "Use the function to split `x_hourly` and `y_hourly_temp`, using `fraction = 0.2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data <- ?\n",
    "\n",
    "x_train = split_data$x_train\n",
    "y_train = split_data$y_train\n",
    "x_test = split_data$x_test\n",
    "y_test = split_data$y_test\n",
    "dim(x_train)\n",
    "length(y_train)\n",
    "dim(x_test)\n",
    "length(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling\n",
    "To help our network train faster, we need to scale our data. As in the previous weeks, we use the training set to scale the training and test sets. To scale a dataset we first needed to compute the mean and standard deviation, using only the training set. We use the [`apply()`](https://www.rdocumentation.org/packages/base/versions/3.5.3/topics/apply) function for this.\n",
    "\n",
    "## Exercise 5\n",
    "Fill in the apply function below to compute the mean and standard deviation of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the mean and sd for each feature independently.\n",
    "mean <- apply(?, ?, mean)\n",
    "std <- apply(?, ?, sd)\n",
    "\n",
    "mean[2] # The mean temperature\n",
    "mean[1] # The mean air pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the mean and sd for each feature independently.\n",
    "mean <- apply(x_train, 3, mean)\n",
    "std <- apply(x_train, 3, sd)\n",
    "\n",
    "mean[2] # The mean temperature\n",
    "mean[1] # The mean air pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week, we cannot use the `scale()` function since it only works with (at most) 2D tensors, so we need to do this manually. Since we already split up the dataset into train/test, we need to scale both of them. We store the result in `x_train_scaled` and `x_test_scaled`. We provide the code for this, but we recommend reading through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty 3D tensor with same dimensions as feature_matrix_reshaped.\n",
    "x_train_scaled <- array(0, dim = dim(x_train))\n",
    "x_test_scaled <- array(0, dim = dim(x_test))\n",
    "\n",
    "# For each feature we subtract the mean and divide by the std.\n",
    "for (j in 1:dim(x_train)[3]) {\n",
    "    x_train_scaled[,,j] <- (x_train[,,j] - mean[j]) / std[j]\n",
    "    x_test_scaled[,,j] <- (x_test[,,j] - mean[j]) / std[j]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if this all makes sense and check the first temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[1,1,2]\n",
    "x_train_scaled[1,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The models\n",
    "Last week we used a dense network to serve as our baseline, this week we will use a SimpleRNN and then see if a GRU or an LSTM helps improve our predictions.\n",
    "\n",
    "Notice that instead of providing validation data, we simply specify the fraction of training data to use for validation using the `validation_split = 0.2` parameter in the `fit()` function. According to the Keras documentation the [`fit()`](https://keras.io/models/sequential/) function will use the last elements of the training data (similar to what we have done for the test set). This is what we want, since we do not want to validate our data from the past. Also notice that our test data is even further ahead in the future.\n",
    "\n",
    "## Exercise 6.1\n",
    "- Create a network with a `simple_rnn_layer` with 16 units.\n",
    "- After the simple RNN, create a regression layer.\n",
    "- Use mae loss.\n",
    "The `simple_rnn_layer` will need the `input_shape` parameter since it is the first layer in our network. We have shaped our input so that it has a sequence length of 96 and 14 features. You need to provide these values as the `input_shape`.\n",
    "\n",
    "**Hint** To speed up training it is helpful to remove the `callbacks=list(Progress$new())` line from the `fit` function. The side-effect is that you have less idea of how far in the training process you are, but you can be sure that it is about 1/5 faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "    ?\n",
    "\n",
    "cat(summary(model))\n",
    "\n",
    "model %>% compile(\n",
    "    optimizer = optimizer_adam(lr = 0.001),\n",
    "    loss = ?\n",
    ")\n",
    "\n",
    "history <- model %>% fit(\n",
    "    x = x_train_scaled,\n",
    "    y = y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 40,\n",
    "    batch_size = 16,\n",
    "    callbacks=list(Progress$new())\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see an average validation loss of about 4, we also see that the model seems to have started overfitting to the data. Usually, when we start overfitting to our data, we would not try a more expressive model as we would expect to overfit the data even more. Against our intution, let us try to use a GRU layer instead of a simple RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "Let us try a GRU layer instead of the SimpleRNN above. Copy the model above and only change the `layer_simple_rnn` to `layer_gru`. Notice that the GRU takes even longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We see that the GRU actually outperforms the SimpleRNN and does not start overfitting the data. This seems to indicate that there were some long term dependenices which the SimpleRNN was not able to account for and it started overfitting to latest datapoints in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare evaluate our GRU against the test set by running the cell below. We will see that the test loss is similar to our validation loss, which is very good, it means that we have not methodologically overfit to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.3\n",
    "The test loss can be a bit abstract for us, so let us also manually compare the predictions of the model on the test set against the test labels. To get the predictions we use the function `predict()` and pass in the model as first parameter and then the `x_test_scaled` data. The function will output a predictions for each sequence in the test data. Store these predictions in a variable called `predictions`. Then combine the `predictions` column-wise with `y_test` using the function `cbind()`. Then print out the first 10 rows (using `head()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions <- model %>% predict(?)\n",
    "compare <- cbind(?, ?)\n",
    "head(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that \"on average\" we are about 3 degrees off, that is what the MAE loss of 3 means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we saw that the GRU did improve our predictions and it was very easy to adjust our original model to use a GRU instead. In the bonus exercise below, we suggest that you also try an LSTM, they tend to perform even better.\n",
    "\n",
    "In this notebook we also saw three new things. First, we resampled the data so that we had a datapoint every hour using the `seq()` function. Second, we saw a different way to create sequences from a dataset. We created a helpful function for this which we can hopefully reuse in later notebooks. Third, we did not split our data into a validation set as we left it to Keras, which managed it for us in the `fit()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus execrise\n",
    "Copy the model from above and try using an LSTM instead of a GRU. Can you improve the performance. Does adding a dense layer after the LSTM help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
